# -*- coding: utf-8 -*-
"""TCC_SI_Deteccao_de_Phishing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XomBW6Mg4KVQIKvOg2CxoOqzNreYwWds

# Trabalho de Conclusão de Curso (TCC)
## Curso de Segurança da Informação
### Universidade Federal do Ceará (UFC)

**Autor:** *Tiago Andrade*  
**Orientador:** *Prof. Israel Eduardo*  
**Ano:** 2025

---

## Importações e Configurações
"""

!pip install psutil

#Bibliotecas padrão do Python
import re
import time
import tracemalloc
from math import sqrt
from collections import Counter
from itertools import chain

#Bibliotecas científicas e de visualização
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import norm
from IPython.display import display

#NLP e pré-processamento de texto
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from tqdm.notebook import tqdm

#Machine Learning
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, make_scorer,
    ConfusionMatrixDisplay
)

#Cópia de modelos
import copy

#desempenho computacional
import psutil
import os

#Estilo visual
sns.set(style="whitegrid", palette="muted")
pd.set_option('display.max_colwidth', 150)

"""## Carregamento e Exploração dos Dados"""

csv_path = '/content/phishing_email.csv'
df = pd.read_csv(csv_path)
print('Dimensões do dataset:', df.shape)
print('\nColunas disponíveis:', df.columns.tolist())
print('\nValores faltantes por coluna:')
print(df.isnull().sum())
print("\nDistribuição da coluna 'label':")
print(df['label'].value_counts())
display(df.head(5))

#variável binária indicando presença de URL
df['has_url'] = df['text_combined'].str.contains(r'http\S+|www\S+', regex=True).astype(int)

print("Distribuição geral de e-mails com URLs:")
url_dist = df['has_url'].value_counts(normalize=True).round(3) * 100
print(url_dist)

print("\nProporção média de e-mails com URLs por classe:")
url_class_mean = df.groupby('label')['has_url'].mean().round(3) * 100
print(url_class_mean)

#e-mails que possuem link
df_urls = df[df['has_url'] == 1]

#quantos e-mails com URL tem em cada classe
url_counts = df_urls['label'].value_counts().sort_index()

sns.barplot(
    x=['Legítimos', 'Phishing'],
    y=url_counts.values,
    palette=['#6BAED6', '#F03B20']
)
plt.title('Quantidade de e-mails que possuem URLs por classe')
plt.xlabel('Classe')
plt.ylabel('Quantidade de e-mails com links')
plt.show()

"""## Pré-processamento de Texto"""

#modelo de linguagem spaCy
try:
    nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])
except OSError:
    import sys, subprocess
    subprocess.run([sys.executable, '-m', 'spacy', 'download', 'en_core_web_sm'], check=False)
    nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])

#limite de caracteres processáveis
nlp.max_length = 5_000_000

#stopwords personalizadas
custom_stopwords = {
    'enron','aug','email','one','ect','time','submissionid','would',
    'message','note','may','submission','company','com','cnn','cnncom',
    'news','sender','subject','good','add','list','hou','university',
    'total','file','say','thank','year','daily','cable'
}
all_stopwords = {w.lower() for w in STOP_WORDS.union(custom_stopwords)}

#função de pré-processamento
def preprocess_text(text: str) -> str:
    text = str(text)

    #limite do texto
    if len(text) > 200_000:
        text = text[:200_000]

    #removendo caracteres e convertendo para minúsculas
    text = re.sub(r'[^a-zA-Z\s]', ' ', text).lower()

    #tokenização e lematização
    doc = nlp(text)

    #removendo stopwords e palavras curtas
    tokens = [
        token.lemma_.lower() for token in doc
        if token.lemma_.lower() not in all_stopwords
        and len(token.lemma_) > 2
        and token.is_alpha
    ]

    return ' '.join(tokens)

#aplicando o pré-processamento
tqdm.pandas()
df['text_cleaned'] = df['text_combined'].astype(str).progress_apply(preprocess_text)

print('Pré-processamento concluído!')

X = df['text_cleaned']
y = df['label'].astype(int)

"""## Análise Exploratória"""

sns.countplot(x=y)
plt.title('Distribuição das Classes (Phishing vs. Legítimo)')
plt.show()

def top_n_palavras(serie_textos, n=20):
    todas = list(chain.from_iterable(serie_textos.str.split()))
    cont = Counter(todas)
    top = cont.most_common(n)
    return pd.DataFrame(top, columns=['Palavra','Frequência'])

df_top_all = top_n_palavras(df['text_cleaned'], 20)
df_top_phishing = top_n_palavras(df[df['label']==1]['text_cleaned'], 20)
df_top_legitimos = top_n_palavras(df[df['label']==0]['text_cleaned'], 20)
print('Top 20 palavras — Dataset completo'); display(df_top_all)
print('\nTop 20 palavras — E-mails de phishing'); display(df_top_phishing)
print('\nTop 20 palavras — E-mails legítimos'); display(df_top_legitimos)

#dataset completo
plt.figure(figsize=(10,6))
sns.barplot(data=df_top_all, x='Frequência', y='Palavra', palette='Blues_d', orient='h')
plt.title('Top 20 Palavras — Dataset Completo')
plt.xlabel('Frequência')
plt.ylabel('')
plt.show()

#phishing
plt.figure(figsize=(10,6))
sns.barplot(data=df_top_phishing, x='Frequência', y='Palavra', palette='Reds_d', orient='h')
plt.title('Top 20 Palavras — Phishing')
plt.xlabel('Frequência')
plt.ylabel('')
plt.show()

#legítimos
plt.figure(figsize=(10,6))
sns.barplot(data=df_top_legitimos, x='Frequência', y='Palavra', palette='Greens_d', orient='h')
plt.title('Top 20 Palavras — Legítimos')
plt.xlabel('Frequência')
plt.ylabel('')
plt.show()

"""##Divisão Treino/Teste"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, shuffle=True, random_state=42
)
print('Treino:', len(X_train), '| Teste:', len(X_test))

"""##Modelagem (Pipelines)"""

pipe_nb  = make_pipeline(TfidfVectorizer(stop_words='english'), MultinomialNB())
pipe_svm = make_pipeline(TfidfVectorizer(stop_words='english'), LinearSVC())
pipe_rf  = make_pipeline(TfidfVectorizer(stop_words='english'), RandomForestClassifier(n_estimators=100, random_state=42))

modelos = {'MultinomialNB': pipe_nb, 'LinearSVC': pipe_svm, 'RandomForest': pipe_rf}

"""##Validação Cruzada e Intervalos de Confiança"""

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scoring = {
    'accuracy':  make_scorer(accuracy_score),
    'precision': make_scorer(precision_score, pos_label=1),
    'recall':    make_scorer(recall_score,    pos_label=1),
    'f1':        make_scorer(f1_score,        pos_label=1),
}
def avalia_modelo_cv(pipe, X_train, y_train):
    return cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)
cv_nb  = avalia_modelo_cv(pipe_nb,  X_train, y_train)
cv_svm = avalia_modelo_cv(pipe_svm, X_train, y_train)
cv_rf  = avalia_modelo_cv(pipe_rf,  X_train, y_train)

def resumo_ic(scores):
    n = len(scores)
    m = np.mean(scores)
    se = np.std(scores, ddof=1) / np.sqrt(n)
    from scipy.stats import norm
    z = norm.ppf(0.975)
    return m, (m - z*se, m + z*se)

for nome, cvres in [('NB', cv_nb), ('SVM', cv_svm), ('RF', cv_rf)]:
    f1_m, (f1_lo, f1_hi) = resumo_ic(cvres['test_f1'])
    print(f'{nome}: F1 = {f1_m:.3f}  (IC95% {f1_lo:.3f}–{f1_hi:.3f})')

"""##Teste Final, Matrizes de Confusão e Comparativos

"""

def resumo_cv(nome, cvres):
    return {
        'Modelo': nome,
        'Acc (mean)':  np.mean(cvres['test_accuracy']),
        'Prec (mean)': np.mean(cvres['test_precision']),
        'Rec (mean)':  np.mean(cvres['test_recall']),
        'F1 (mean)':   np.mean(cvres['test_f1']),
    }

df_cv = pd.DataFrame([
    resumo_cv('MultinomialNB', cv_nb),
    resumo_cv('LinearSVC', cv_svm),
    resumo_cv('RandomForest', cv_rf)
]).sort_values('F1 (mean)', ascending=False).reset_index(drop=True)
display(df_cv.style.background_gradient(cmap='YlGnBu', axis=0))

#matrizes de confusão
for nome, modelo in modelos.items():
    modelo.fit(X_train, y_train)
    y_pred = modelo.predict(X_test)

    plt.figure(figsize=(6, 4))
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues', colorbar=False)
    plt.title(f"Matriz de Confusão - {nome}")
    plt.tight_layout()
    plt.show()

df_grafico = df_cv.rename(columns={
    'Acc (mean)': 'Acurácia',
    'Prec (mean)': 'Precisão',
    'Rec (mean)': 'Recall',
    'F1 (mean)': 'F1-Score'
})

plt.figure(figsize=(10,6))

melt = df_grafico.melt(id_vars='Modelo', var_name='Métrica', value_name='Valor')

sns.barplot(data=melt, x='Métrica', y='Valor', hue='Modelo', palette='Blues_d')

plt.title('Comparação de Desempenho dos Modelos (Validação Cruzada)')
plt.ylim(0,1)
plt.ylabel('Valor da Métrica')
plt.xlabel('Métrica de Desempenho')

plt.legend(
    title='Modelo',
    bbox_to_anchor=(1.05, 1),
    loc='upper left'
)

plt.tight_layout()
plt.show()

"""##Desempenho Computacional"""

def avaliar_desempenho_real(nome, modelo, X_train, y_train, X_test, y_test):

    processo = psutil.Process(os.getpid())

    mem_inicio_rss = processo.memory_info().rss

    t0 = time.time()
    modelo.fit(X_train, y_train)
    t1 = time.time()

    mem_treino_rss = processo.memory_info().rss

    t2 = time.time()
    y_pred = modelo.predict(X_test)
    t3 = time.time()

    mem_pred_rss = processo.memory_info().rss


    uso_mem_treino = (mem_treino_rss - mem_inicio_rss) / (1024 * 1024)

    uso_mem_pred = (mem_pred_rss - mem_treino_rss) / (1024 * 1024)

    return {
        'Modelo': nome,
        'Tempo Treino (s)': t1 - t0,
        'Tempo Pred (s)': t3 - t2,
        'Memória Treino (MB)': uso_mem_treino,
        'Memória Pred (MB)': uso_mem_pred,
        'F1-Score': f1_score(y_test, y_pred)
    }

avaliacoes = [
    avaliar_desempenho_real('MultinomialNB', copy.deepcopy(modelos['MultinomialNB']), X_train, y_train, X_test, y_test),
    avaliar_desempenho_real('LinearSVC', copy.deepcopy(modelos['LinearSVC']), X_train, y_train, X_test, y_test),
    avaliar_desempenho_real('RandomForest', copy.deepcopy(modelos['RandomForest']), X_train, y_train, X_test, y_test)
]
df_comp = pd.DataFrame(avaliacoes).sort_values('F1-Score', ascending=False)
display(df_comp.style.background_gradient(cmap='YlGnBu'))

plt.figure(figsize=(14,5))
plt.subplot(1,2,1)
sns.barplot(data=df_comp, x='Modelo', y='Tempo Treino (s)', palette='Blues_d')
plt.title('Tempo de Treinamento por Modelo'); plt.ylabel('Segundos'); plt.xlabel('')

plt.subplot(1,2,2)
sns.barplot(data=df_comp, x='Modelo', y='Memória Treino (MB)', palette='Greens_d')
plt.title('Memória Utilizada no Treinamento por Modelo'); plt.ylabel('Megabytes (MB)'); plt.xlabel('')

plt.tight_layout(); plt.show()

!python --version
